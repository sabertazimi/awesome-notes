"use strict";(self.webpackChunknotes=self.webpackChunknotes||[]).push([[7199],{81836:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"cs/architecture/architecture","title":"Architecture","description":"SuperPipeline and SuperScalar","source":"@site/content/cs/architecture/architecture.md","sourceDirName":"cs/architecture","slug":"/cs/architecture/","permalink":"/notes/cs/architecture/","draft":false,"unlisted":false,"editUrl":"https://github.com/sabertazimi/notes/edit/main/content/cs/architecture/architecture.md","tags":[{"inline":true,"label":"CS","permalink":"/notes/tags/cs"},{"inline":true,"label":"Architecture","permalink":"/notes/tags/architecture"}],"version":"current","lastUpdatedBy":"Sabertaz","lastUpdatedAt":1769518084000,"frontMatter":{"tags":["CS","Architecture"]},"sidebar":"tutorialSidebar","previous":{"title":"Simulation","permalink":"/notes/cs/algorithms/simulation"},"next":{"title":"Compiler","permalink":"/notes/cs/compilers/"}}');var i=s(35656),r=s(86145);const a={tags:["CS","Architecture"]},o="Architecture",c={},l=[{value:"SuperPipeline and SuperScalar",id:"superpipeline-and-superscalar",level:2},{value:"Instructions Dependencies and Latencies",id:"instructions-dependencies-and-latencies",level:3},{value:"Branch Prediction",id:"branch-prediction",level:3},{value:"VLIW",id:"vliw",level:2},{value:"Out of Order Execution",id:"out-of-order-execution",level:2},{value:"The Brainiac vs Speed-demon Debate",id:"the-brainiac-vs-speed-demon-debate",level:3},{value:"Power Wall and ILP Wall",id:"power-wall-and-ilp-wall",level:3},{value:"Decoupled x86 micro-architecture",id:"decoupled-x86-micro-architecture",level:3},{value:"SMT",id:"smt",level:2},{value:"More cores or Wider cores",id:"more-cores-or-wider-cores",level:3},{value:"DLP",id:"dlp",level:2},{value:"SIMD Vector Instructions",id:"simd-vector-instructions",level:3},{value:"Memory",id:"memory",level:2},{value:"Wall",id:"wall",level:3},{value:"Caches",id:"caches",level:3},{value:"Locality",id:"locality",level:4},{value:"Layout",id:"layout",level:4},{value:"Latency and Bandwidth",id:"latency-and-bandwidth",level:3},{value:"Distributed System",id:"distributed-system",level:2},{value:"References",id:"references",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"architecture",children:"Architecture"})}),"\n",(0,i.jsx)(n.h2,{id:"superpipeline-and-superscalar",children:"SuperPipeline and SuperScalar"}),"\n",(0,i.jsx)(n.p,{children:"A superpipeline processor own a 5-20 stage pipeline,\na superscalar issues 3-8 instructions in parallel\n(more functional units like integer/float units)."}),"\n",(0,i.jsx)(n.h3,{id:"instructions-dependencies-and-latencies",children:"Instructions Dependencies and Latencies"}),"\n",(0,i.jsxs)(n.p,{children:["The number of cycles between when an instruction reaches the execute stage\nand when its result is available for use by other instructions is called the\ninstruction's latency.\nThe deeper the pipeline, the more stages and thus the longer the ",(0,i.jsx)(n.strong,{children:"latency"}),".\nThe processor will need to stall the execution of the instructions until\ntheir data is available (",(0,i.jsx)(n.strong,{children:"dependencies"}),"),\ninserting a bubble into the pipeline where no work gets done, making multiple\nissue in this case impossible."]}),"\n",(0,i.jsx)(n.h3,{id:"branch-prediction",children:"Branch Prediction"}),"\n",(0,i.jsx)(n.p,{children:"Another key problem for pipelining is branches\n(flush out all instructions of wrong branch).\nUnfortunately, even the best branch prediction techniques are sometimes wrong,\nand with a deep pipeline many instructions might need to be cancelled.\nThis is called the mispredict penalty.\nThe deeper the pipeline, the further into the future you must try to predict,\nthe more likely you'll be wrong,\nand the greater the mispredict penalty when you are."}),"\n",(0,i.jsxs)(n.p,{children:["Predication can be used to eliminate branches such as ",(0,i.jsx)(n.code,{children:"cmovle"}),"\n(move data only when testing flag stays less or equal state).\nThe Alpha architecture had a conditional move instruction from the very beginning.\nMIPS, SPARC, x86 added it later and the ARM architecture\nwas the first architecture with a ",(0,i.jsx)(n.strong,{children:"fully predicated"})," instruction set\n(though the early ARM processors only had short pipelines and small mispredict penalties)."]}),"\n",(0,i.jsx)(n.h2,{id:"vliw",children:"VLIW"}),"\n",(0,i.jsx)(n.p,{children:"Very long instruction word:\nIn cases where backward compatibility is not an issue,\nit is possible for the instruction set itself to be designed\nto explicitly group instructions to be executed in parallel.\nA VLIW processor's instruction flow is much like a super-scalar,\nexcept the decode/dispatch stage is much simpler\nand only occurs for each group of sub-instructions.\nNo VLIW designs have yet been commercially successful as mainstream CPUs."}),"\n",(0,i.jsx)(n.h2,{id:"out-of-order-execution",children:"Out of Order Execution"}),"\n",(0,i.jsxs)(n.p,{children:["If branches and long-latency instructions are going to cause bubbles in the pipeline(s),\nthen perhaps those empty cycles can be used to do other work.\nTo achieve this, the instructions in the program must be reordered\n(",(0,i.jsx)(n.strong,{children:"instruction scheduling"})," and ",(0,i.jsx)(n.strong,{children:"register renaming"}),").\nCompiler completes static instruction scheduling\n(rearranged instruction stream at compile time),\nprocessor completes dynamic instruction scheduling\n(renaming registers and reorder instruction stream at runtime).\nThe processor must keep a mapping of the instructions\nin flight at any moment and the physical registers they use.\nThe extra logic of scheduler is particularly ",(0,i.jsx)(n.strong,{children:"power-hungry"}),"\nbecause those transistors are ",(0,i.jsx)(n.strong,{children:"always"})," working."]}),"\n",(0,i.jsx)(n.h3,{id:"the-brainiac-vs-speed-demon-debate",children:"The Brainiac vs Speed-demon Debate"}),"\n",(0,i.jsx)(n.p,{children:"Brainiac designs are at the smart-machine end of the spectrum,\nwith lots of OOO hardware trying to squeeze every last drop of\ninstruction-level parallelism out of the code,\neven if it costs millions of logic transistors and years of design effort to do it.\nIn contrast, speed-demon designs are simpler and smaller,\nrelying on a smart compiler and willing to sacrifice a little bit of\ninstruction-level parallelism\nfor the other benefits that simplicity brings.\nWhich would you rather have: 4 powerful brainiac cores, or 8 simpler in-order cores?\nWhen it comes to the brainiac debate,\nmany vendors have gone down one path then changed their mind and switched to\nthe other side."}),"\n",(0,i.jsx)(n.h3,{id:"power-wall-and-ilp-wall",children:"Power Wall and ILP Wall"}),"\n",(0,i.jsxs)(n.p,{children:["Power usage goes up even faster than clock speed does\n(increasing clock speed by 20% with 50% more power usage,\nO(power) = ",(0,i.jsx)(n.code,{children:"frequency * Voltage * Voltage"}),").\nLeakage current also goes up as the voltage is increased,\nleakage generally goes up as the temperature increases as well.\nThe power and heat problems become unmanageable,\nbecause it's simply not possible to provide that much power and cooling to a\nsilicon chip.\nThus, going purely for clock speed is not the best strategy."]}),"\n",(0,i.jsx)(n.p,{children:"normal programs just don't have a lot of fine-grained parallelism in them,\ndue to a combination of load latencies, cache misses,\nbranches and dependencies between instructions.\nThis limit of available instruction-level parallelism is called the ILP wall."}),"\n",(0,i.jsx)(n.h3,{id:"decoupled-x86-micro-architecture",children:"Decoupled x86 micro-architecture"}),"\n",(0,i.jsxs)(n.p,{children:['Dynamically decode the x86 instructions into simple,\nRISC-like micro-instructions (\u03bcops, pronounced "micro-ops"),\nwhich can then be executed by a fast,\nRISC-style register-renaming OOO superscalar core.\nThe pipeline depth of Core i',(0,i.jsx)(n.em,{children:"2/i"}),"3 Sandy/Ivy Bridge\nwas shown as 14/19 stages in the earlier section on superpipeline,\nit is 14 stages when the processor is running from its L0 \u03bcop cache\n(which is the common case),\nbut 19 stages when running from the L1 instruction cache\nand having to decode x86 instructions and translate them into \u03bcops."]}),"\n",(0,i.jsx)(n.h2,{id:"smt",children:"SMT"}),"\n",(0,i.jsx)(n.p,{children:"Hardware threads:\nEven the most aggressively brainiac OOO superscalar processor\nwill still almost never exceed an average of about 2-3 instructions per cycle\nwhen running most mainstream, real-world software,\ndue to a combination of load latencies, cache misses,\nbranching and dependencies between instructions."}),"\n",(0,i.jsxs)(n.p,{children:["Simultaneous multi-threading (",(0,i.jsx)(n.strong,{children:"SMT"}),") is a processor design technique\nwhich exploits thread-level parallelism\n(other running programs, or other threads within the same program).\nThe instructions come from multiple threads running at the same time,\nall on the one processor core.\nAn SMT processor uses just one physical processor core\nto present two or more logical processors to the system.\nSeparate units include the program counter, the architecturally-visible registers,\nthe memory mappings held in the TLB,\nshared units include the decoders and dispatch logic,\nthe functional units, and the caches.\nSMT is essentially a way to ",(0,i.jsx)(n.strong,{children:"convert TLP into ILP"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["However, in practice, at least for desktops, laptops, tablets, phones and small servers,\nit is rarely the case that several different programs\nare actively executing at the same time,\nso it usually comes down to just the ",(0,i.jsx)(n.strong,{children:"one task"}),"\nthe machine is currently being used for.\nSome applications, such as database systems, image and video processing,\naudio processing, 3D graphics rendering and scientific code,\ndo have obvious high-level (coarse-grained) parallelism available and easy to exploit,\nbut many of these applications which are easy\nto parallelize are primarily limited by ",(0,i.jsx)(n.strong,{children:"memory bandwidth"}),", not by the processor."]}),"\n",(0,i.jsxs)(n.p,{children:["If one thread saturates just one functional unit which the other threads need,\nit effectively stalls all of the other threads,\neven if they only need relatively little use of that unit.\n",(0,i.jsx)(n.strong,{children:"Competition"})," between the threads for cache space may produce worse results\nthan letting just one thread have all the cache space available,\nparticularly for software where the critical working set is highly cache-size sensitive,\nsuch as hardware simulators/emulators, virtual machines and high-quality video encoding."]}),"\n",(0,i.jsx)(n.p,{children:"Due to above 3 reasons, SMT performance can actually\nbe worse than single-thread performance\n(traditional context switching between threads) sometimes."}),"\n",(0,i.jsx)(n.h3,{id:"more-cores-or-wider-cores",children:"More cores or Wider cores"}),"\n",(0,i.jsx)(n.p,{children:"Very wide superscalar designs scale very badly\nin terms of both chip area and clock speed,\nso a single 10-issue core would actually\nbe both larger and slower than two 5-issue cores:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"the complex multiple-issue dispatch logic scales up as (issue width)^2"}),"\n",(0,i.jsx)(n.li,{children:"highly multi-ported register files\nand caches to service all those simultaneous accesses"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:['Today, a "typical" SMT design implies both a wide execution core\nand OOO execution logic,\nincluding multiple decoders,\nthe large and complex superscalar dispatch logic and so on.\nFor applications with lots of active but ',(0,i.jsx)(n.strong,{children:"memory-latency-limited"})," threads\n(database systems, 3D graphics rendering),\nmore ",(0,i.jsx)(n.strong,{children:"simple cores"})," would be better\nbecause big/wide cores would spend most of their time waiting for memory anyway.\nFor ",(0,i.jsx)(n.strong,{children:"most applications"}),", however,\nthere simply are not enough threads active to make this viable,\nand the performance of just a single thread is much more important,\nso a design with ",(0,i.jsx)(n.strong,{children:"fewer but bigger, wider"}),", more brainiac cores is more appropriate."]}),"\n",(0,i.jsxs)(n.p,{children:["Intel's Xeon Haswell, the server version of Core ",(0,i.jsx)(n.code,{children:"i*4"})," Haswell,\nuses 5.7 billion transistors to provide 18 cores (up from 8 in Xeon Sandy Bridge),\neach a very aggressively brainiac 8-issue design (up from 6-issue in Sandy Bridge),\neach still with 2-thread SMT.\nIBM's POWER8 uses 4.4 billion transistors to\nmove to a considerably more brainiac core design than POWER7,\nand at the same time provide 12 cores (up from 8 in POWER7),\neach with 8-thread SMT (up from 4 in POWER7)."]}),"\n",(0,i.jsxs)(n.p,{children:["In the future we might see ",(0,i.jsx)(n.strong,{children:"asymmetric designs"}),',\nwith one or two big, wide,\nbrainiac cores plus a large number of smaller, narrower, simpler cores.\nIBM\'s Cell processor (used in the Sony PlayStation 3)\nwas arguably the first such design,\nbut unfortunately it suffered from severe programmability problems\nbecause the ISA incompatible between small cores and large main core\nand had limited by awkward access to main memory.\nSome modern ARM designs also use an asymmetric approach,\nwith several large cores paired with one or a few smaller, simpler "companion" cores,\nto increase battery life.']}),"\n",(0,i.jsx)(n.h2,{id:"dlp",children:"DLP"}),"\n",(0,i.jsx)(n.p,{children:"data-level parallelism:\nRather than looking for ways to execute groups of instructions in parallel,\nthe idea is to look for ways to make one instruction\napply to a group of data values in parallel."}),"\n",(0,i.jsx)(n.h3,{id:"simd-vector-instructions",children:"SIMD Vector Instructions"}),"\n",(0,i.jsxs)(n.p,{children:["One of DLP methods called ",(0,i.jsx)(n.strong,{children:"SIMD"})," parallelism (single instruction, multiple data).\nMore often, it's called ",(0,i.jsx)(n.strong,{children:"vector processing"}),".\nWith some thought, a small set of vector instructions\ncan enable some impressive speedups,\nsuch as packing/unpacking, byte shuffling, bit masking instructions,\njust like x86 Matrix Math Extensions (MMX),\nStreaming SIMD Extensions (SSE),\nand ongoing revisions of Advanced Vector Extensions (AVX).\nMMX provide 64-bit vectors, x86 SSE added 8 new 128-bit registers,\nthen widened to 256 bits with AVX."]}),"\n",(0,i.jsx)(n.h2,{id:"memory",children:"Memory"}),"\n",(0,i.jsx)(n.h3,{id:"wall",children:"Wall"}),"\n",(0,i.jsxs)(n.p,{children:["Latency is especially bad for loads from memory,\nwhich make up about a quarter of all instructions.\nUsing a modern SDRAM with a CAS latency of 11,\nwill typically be ",(0,i.jsx)(n.strong,{children:"24 cycles of the memory system bus"}),",\n1 to send the address to the DIMM (memory module),\nRAS-to-CAS delay of 11 for the row access,\nCAS latency of 11 for the column access,\nand a final 1 to send the first piece of data up to the processor (or E-cache).\nOn a multi-processor system, even more bus cycles\nmay be required to support ",(0,i.jsx)(n.strong,{children:"cache coherency"})," between the processors.\nThere are the cycles within the processor itself,\nchecking the various on-chip caches before the address\neven gets sent to the memory controller, accounting for ",(0,i.jsx)(n.strong,{children:"20 CPU cycles"}),".\nFor 2.4GHz processor and 800MHz SDRAM memory,\nsumming up to ",(0,i.jsx)(n.code,{children:"(1+11+11+1) * 2400/800 + 20 = 92"})," CPU cycles,\na 4.0 GHz processor would wait a staggering 140 cycles to access main memory.\nThis problem of the large, and slowly growing,\ngap between the processor and main memory is called the memory wall."]}),"\n",(0,i.jsx)(n.h3,{id:"caches",children:"Caches"}),"\n",(0,i.jsxs)(n.p,{children:["Modern processors solve the problem of the memory wall with caches.\nA cache is a small but fast type of memory located on or near the processor chip.\nIts role is to keep ",(0,i.jsx)(n.strong,{children:"faster copies"})," of small pieces of main memory,\nL1 caches around 8-64K in size, L2 caches around 100K-10M in size,\nlarger and slower L3 caches.\nA modern primary (L1) cache has a latency of just 2 to 4 processor cycles,\nwith around 90% caches hit rates."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"The memory hierarchy of a modern desktop/laptop: Core i4 Haswell."}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Level"}),(0,i.jsx)(n.th,{children:"Size"}),(0,i.jsx)(n.th,{children:"Latency (cycles)"}),(0,i.jsx)(n.th,{children:"Location"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"L1 Cache"}),(0,i.jsx)(n.td,{children:"32KB"}),(0,i.jsx)(n.td,{children:"4"}),(0,i.jsx)(n.td,{children:"inside each core"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"L2 Cache"}),(0,i.jsx)(n.td,{children:"256KB"}),(0,i.jsx)(n.td,{children:"12"}),(0,i.jsx)(n.td,{children:"beside each core"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"L3 Cache"}),(0,i.jsx)(n.td,{children:"6MB"}),(0,i.jsx)(n.td,{children:"~21"}),(0,i.jsx)(n.td,{children:"shared between all cores"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"L4 E-Cache"}),(0,i.jsx)(n.td,{children:"128MB"}),(0,i.jsx)(n.td,{children:"~58"}),(0,i.jsx)(n.td,{children:"separate eDRAM chip"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RAM"}),(0,i.jsx)(n.td,{children:"8+GB"}),(0,i.jsx)(n.td,{children:"~117"}),(0,i.jsx)(n.td,{children:"SDRAM DIMMs on motherboard"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Swap"}),(0,i.jsx)(n.td,{children:"100+GB"}),(0,i.jsx)(n.td,{children:"10000+"}),(0,i.jsx)(n.td,{children:"hard disk or SSD"})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"locality",children:"Locality"}),"\n",(0,i.jsx)(n.p,{children:"Temporal locality is exploited by merely keeping recently accessed data in the cache.\nTo take advantage of spatial locality,\ndata is transferred from main memory up into the cache\nin blocks of a few dozen bytes at a time, called a cache line."}),"\n",(0,i.jsx)(n.h4,{id:"layout",children:"Layout"}),"\n",(0,i.jsxs)(n.p,{children:["Using the virtual address might cause caches\nneed to be flushed on every context switch (",(0,i.jsx)(n.strong,{children:"thrashing"}),")\n(2 programs mapping a same virtual address to different physical address).\nUsing the physical address means the V2N mapping must be performed,\nmaking every cache lookup slower.\nA common trick is to use virtual addresses\nfor the cache indexing but physical addresses for the tags.\nThe virtual-to-physical mapping (TLB lookup) can then\nbe performed in parallel with the cache indexing\nso that it will be ready in time for the tag comparison.\nSuch a scheme is called a virtually-indexed physically-tagged cache."]}),"\n",(0,i.jsx)(n.p,{children:'Set-associative caches are able to avoid some unfortunate cache conflicts.\nUnfortunately, the more highly associative a cache is, the slower it is to access.\nThe instruction L1 cache can afford to be highly set-associative\n(prefetching and buffering in pipeline),\nbut the data L1 cache settled on 4-way set-associative as the sweet spot.\nThe large L2/L3 cache (LLC for "last-level cache")\nis also usually highly associative, perhaps as much as 12- or 16-way.\nExternal E-cache is sometimes direct-mapped for flexibility of size and implementation.'}),"\n",(0,i.jsx)(n.h3,{id:"latency-and-bandwidth",children:"Latency and Bandwidth"}),"\n",(0,i.jsx)(n.p,{children:"Lower-latency designs will be better for pointer-chasing code,\nsuch as compilers and database systems.\nBandwidth-oriented (adding more memory banks and making the busses wider)\nsystems have the advantage for programs with simple, linear access patterns,\nsuch as image processing and scientific code."}),"\n",(0,i.jsx)(n.p,{children:"Latency is much harder to improve than bandwidth.\nSynchronously clocked DRAM (SDRAM) allowed pipelining of the memory system.\nThis reduces effective latency because it allows\na new memory access to be started before the current one has completed,\nwhile an asynchronous memory system had\nto wait for the transfer of half a cache line\nfrom the previous access before starting a new request."}),"\n",(0,i.jsx)(n.h2,{id:"distributed-system",children:"Distributed System"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u5c0f\u578b\u673a\u662f\u4e13\u95e8\u8bbe\u8ba1\u7684\u786c\u4ef6\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u8f6f\u4ef6\uff0c\u53ea\u9762\u5411\u8fd9\u79cd\u89c4\u6a21\uff08\u4f8b\u5982\u51e0\u767e\u9897 CPU\uff09\u7684\u8ba1\u7b97"}),"\n",(0,i.jsx)(n.li,{children:"\u5c0f\u578b\u673a\u662f\u5b8c\u5168\u95ed\u6e90\u7684\uff0c\u4e0d\u9700\u8981\u8003\u8651\u6269\u5c55\u6027\uff0c\u7279\u5b9a\u7684\u51e0\u79cd\u786c\u4ef6\u5728\u7a33\u5b9a\u6027\u4e0a\u524d\u8fdb\u4e86\u4e00\u5927\u6b65"}),"\n",(0,i.jsx)(n.li,{children:"x86 \u7684 IO \u6027\u80fd\u88ab\u67b6\u6784\u9501\u6b7b\u4e86\uff0c\u5404\u79cd\u603b\u7ebf\u3001PCI\u3001PCIe\u3001USB\u3001SATA\u3001\u4ee5\u592a\u7f51\uff0c\u4e3a\u4e86\u4e2a\u4eba\u8ba1\u7b97\u673a\u7684\u4fbf\u5229\u6027\uff0c\u727a\u7272\u4e86\u5f88\u591a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027"}),"\n",(0,i.jsx)(n.li,{children:"\u5c0f\u578b\u673a\u4f7f\u7528\u603b\u7ebf\u901a\u4fe1\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6781\u9ad8\u7684\u4fe1\u606f\u4f20\u9012\u6548\u7387\uff0c\u6781\u5176\u6709\u6548\u7684\u76d1\u63a7\u4ee5\u53ca\u6781\u9ad8\u7684\u6545\u969c\u9694\u79bb\u901f\u5ea6"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["x86 \u670d\u52a1\u5668\u57fa\u4e8e",(0,i.jsx)(n.code,{children:"\u7f51\u7edc\u7684\u5206\u5e03\u5f0f"}),"\u5177\u6709\u5929\u7136\u7684\u7f3a\u9677:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u64cd\u4f5c\u7cfb\u7edf\u51b3\u5b9a\u4e86\u7f51\u7edc\u6027\u80fd\u4e0d\u8db3"}),"\n",(0,i.jsx)(n.li,{children:"\u7f51\u7edc\u9700\u8981\u4f7f\u7528\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\uff0c\u6bd4\u603b\u7ebf\u7535\u8def\u7684\u5ef6\u8fdf\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7"}),"\n",(0,i.jsx)(n.li,{children:"PC \u673a\u7684\u786c\u4ef6\u4e0d\u591f\u53ef\u9760\uff0c\u6545\u969c\u7387\u9ad8"}),"\n",(0,i.jsx)(n.li,{children:"\u5f88\u96be\u6709\u6548\u76d1\u63a7\uff0c\u9694\u79bb\u6545\u969c\u901f\u5ea6\u6162"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Modern microprocessors: a 90-minute ",(0,i.jsx)(n.a,{href:"http://www.lighterra.com/papers/modernmicroprocessors",children:"guide"}),"."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},86145:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var t=s(57140);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);